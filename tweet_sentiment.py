# -*- coding: utf-8 -*-
"""Tweet_Sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12KD-S5zQU9ls9E05z0zoEf909xF_AQ01
"""

! pip install kaggle
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle competitions download -c nlp-getting-started
!pip install mglearn

#imports
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import mglearn

trainData = pd.read_csv("/content/train.csv")
testData = pd.read_csv("/content/test.csv")

#looking at data
trainData

testData

print(trainData[["keyword"]].value_counts())
print("\n")
print(trainData[["location"]].value_counts())

#looking at keyword distribution
print("Number of tweets: \n{}\n Null keywords: \n{}\n Non-null keywords: \n{}".format(trainData.sum(), trainData['keyword'].isna().sum(), trainData['keyword'].notna().sum())) 
print("Null location: \n{}\n Non-null location: \n{}".format(trainData['location'].isna().sum(), trainData['location'].notna().sum()))

trainData.describe()

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
stopwords=list(ENGLISH_STOP_WORDS)
stopwords.append("make")
stopwords.append("https")
stopwords.append("http")
print(stopwords)

'''from sklearn.feature_extraction.text import CountVectorizer
tweetTrainList = trainData["text"].tolist()

vect = CountVectorizer(min_df=15, stop_words=stopwords,  ngram_range=(1,2)).fit(tweetTrainList)
X_train = vect.transform(tweetTrainList)
print("X_train:\n{}".format(repr(X_train)))

feature_names = vect.get_feature_names()
print("Number of features: {}".format(len(feature_names)))
print("Every 200th feature: {}".format(feature_names[::100]))

'''

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None, ngram_range=(1,3)), LogisticRegression(max_iter = 10000))
param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}
grid = GridSearchCV(pipe, param_grid, cv=5)
grid.fit(trainData["text"], trainData["target"])
print("Best cross-validation score: {:.2f}".format(grid.best_score_))

vectorizer = grid.best_estimator_.named_steps["tfidfvectorizer"]
# transform the training dataset
X_train = vectorizer.transform(trainData["text"])
# find maximum value for each of the features over the dataset
max_value = X_train.max(axis=0).toarray().ravel()
sorted_by_tfidf = max_value.argsort()

# get feature names
feature_names = np.array(vectorizer.get_feature_names())
print(feature_names)
print("Features with lowest tfidf:\n{}".format(
 feature_names[sorted_by_tfidf[:20]]))
print("Features with highest tfidf: \n{}".format(
 feature_names[sorted_by_tfidf[-20:]]))

coeffs = grid.best_estimator_.named_steps["logisticregression"].coef_
print(coeffs)
coeffList = np.sort(coeffs.squeeze())
print(type(coeffList))
print(len(feature_names))

#lowest tfidf
print("\nLowest tdfif coefficients and words")
for i in range(0, 20):
  print(str(coeffList[i]) + " " + str(feature_names[sorted_by_tfidf[:20]][i]))

#highest
print("\nHighest tdfif coefficients and words")

for i in range(0, 20):
  print(str(coeffList[::-1][i]) + " " + str(feature_names[sorted_by_tfidf[20:]][i]))

feature_names.shape

mglearn.tools.visualize_coefficients(grid.best_estimator_.named_steps["logisticregression"].coef_, feature_names, n_top_features = 40)



'''
# plot them


coef = coeffList.ravel()
positive_coefficients = np.argsort(coef)[-20:]
negative_coefficients = np.argsort(coef)[:20]
interesting_coefficients = np.hstack([negative_coefficients,
                                      positive_coefficients])

plt.figure(figsize=(15, 5))
colors = ["blue" if c < 0 else "red"
          for c in coef[interesting_coefficients]]
plt.bar(np.arange(2 * 20), coef[interesting_coefficients],
        color=colors)

plt.subplots_adjust(bottom=0.3)
plt.xticks(np.arange(1, 1 + 2 * 20),
            feature_names, rotation=60,
            ha="right")
plt.ylabel("Coefficient magnitude")
plt.xlabel("Feature")'''

mglearn.tools.visualize_coefficients(grid.best_estimator_.named_steps["logisticregression"].coef_, feature_names, n_top_features = 40)

print(feature_names)

#Get top and bottom 20 coefficient names (X)
#                                  values (Y)
feature_values = np.array(vectorizer.g)
pltX = feature_names[sorted_by_tfidf[:20]] + feature_names[sorted_by_tfidf[20:]]
pltY = 




fig, ax = plt.subplots()
p1 = ax.bar()

# training the model on training set
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(trainData["text"], trainData["target"])

# making predictions on the testing set
y_pred = gnb.predict(testData["text"])

# comparing actual response values (y_test) with predicted response values (y_pred)
from sklearn import metrics
print("Gaussian Naive Bayes model accuracy(in %):", metrics.accuracy_score(testData["target"], y_pred)*100)